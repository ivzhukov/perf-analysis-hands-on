"use strict";(self.webpackChunkperf_analysis=self.webpackChunkperf_analysis||[]).push([[9381],{1504:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var r=i(4848),t=i(8453);const o={sidebar_position:3},s="Filtering",a={id:"filtering",title:"Filtering",description:"Congratulations, we have made our first measurement with Score-P. But how good was the measurement?",source:"@site/docs/filtering.md",sourceDirName:".",slug:"/filtering",permalink:"/perf-analysis-hands-on/docs/filtering",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Instrumentation",permalink:"/perf-analysis-hands-on/docs/instrumentation"},next:{title:"Explore profile with CUBE",permalink:"/perf-analysis-hands-on/docs/profile_exploration"}},c={},l=[];function m(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"filtering",children:"Filtering"}),"\n",(0,r.jsx)(n.p,{children:"Congratulations, we have made our first measurement with Score-P. But how good was the measurement?\nThe measured execution gave the desired valid result, but the execution took a bit longer than expected! The instrumented run has a large increase in runtime compared to a baseline (around 46s versus 14s). Your runtime may vary slightly from our measurements. Even if we ignore the start and end of the measurement, it was probably prolonged by the instrumentation/measurement overhead."}),"\n",(0,r.jsx)(n.p,{children:"To make sure you don't draw the wrong conclusions based on data that has been disturbed by significant overhead, it's often a good idea to optimise the measurement configuration before you do any more experiments. There are lots of ways you can do this, for example, by using runtime filtering, selective recording, or manual instrumentation to control the measurement."}),"\n",(0,r.jsx)(n.p,{children:'However, in many cases, it\'s enough to filter a few frequently executed but otherwise unimportant user functions to reduce the measurement overhead to an acceptable level (based on experience, we consider 0-20% of runtime dilation as acceptable). The selection of those routines has to be done with care, though, as it affects the granularity of the measurement and too aggressive filtering might "blur" the location of important hotspots.'}),"\n",(0,r.jsx)(n.p,{children:"To understand where the overhead is coming from it is necessary to make scoring of the measurement. It can be done via the following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ scorep-score scorep_bt-mz_sum/profile.cubex \n"})}),"\n",(0,r.jsx)(n.p,{children:"As an output you will see the following:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Estimated aggregate size of event trace:                   160GB\nEstimated requirements for largest trace buffer (max_buf): 6GB\nEstimated memory requirements (SCOREP_TOTAL_MEMORY):       6GB\n(warning: The memory requirements cannot be satisfied by Score-P to avoid\n intermediate flushes when tracing. Set SCOREP_TOTAL_MEMORY=4G to get the\n maximum supported memory or reduce requirements using USR regions filters.)\n\nflt     type    max_buf[B]        visits time[s] time[%] time/visit[us]  region\n         ALL 6,282,548,755 6,586,867,463 5044.19   100.0           0.77  ALL\n         USR 6,265,237,940 6,574,825,097 2257.25    44.7           0.34  USR\n         OMP    17,537,080    10,975,232 2602.86    51.6         237.16  OMP\n         MPI       985,204       339,446  180.12     3.6         530.62  MPI\n         COM       738,530       727,660    3.93     0.1           5.41  COM\n      SCOREP            41            28    0.03     0.0         934.60  SCOREP\n"})}),"\n",(0,r.jsx)(n.p,{children:"As can be seen from the top of the score output, the estimated size for an event trace measurement without filtering applied is approximately 160GB, with the process-local maximum across all ranks being roughly 6GB."}),"\n",(0,r.jsxs)(n.p,{children:["The next section of the score output provides a table which shows how the trace memory requirements of a single process (column ",(0,r.jsx)(n.code,{children:"max_buf"}),") as well as the overall number of visits and CPU allocation time are distributed among certain function groups. In current execution, the following groups are distinguished:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"ALL"}),": All functions of the application."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"MPI"}),": MPI API functions."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"OMP"}),": OpenMP constructs and API functions."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"COM"}),": User functions/regions that appear on a call path to an OpenMP construct, or an OpenMP or MPI API function. Useful to provide the context of MPI/OpenMP usage."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"USR"}),": User functions/regions that do not appear on a call path to an OpenMP construct, or an OpenMP or MPI API function."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"SCOREP"}),": This group aggregates activities within the measurement system."]}),"\n"]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.p,{children:["There are more function groups available, e.g. ",(0,r.jsx)(n.code,{children:"CUDA"}),",",(0,r.jsx)(n.code,{children:"OPENACC"}),",",(0,r.jsx)(n.code,{children:"MEMORY"}),",",(0,r.jsx)(n.code,{children:"IO"}),",",(0,r.jsx)(n.code,{children:"LIB"}),", etc. For more details consult with the documentation ",(0,r.jsx)(n.a,{href:"https://perftools.pages.jsc.fz-juelich.de/cicd/scorep/tags/latest/html/score.html",children:"here"}),"."]})}),"\n",(0,r.jsxs)(n.p,{children:["As we can see from the scoring output, the ",(0,r.jsx)(n.code,{children:"USR"})," group is making the biggest contribution to the trace memory requirements. To figure out which routines are causing the problem, we need to see a breakdown by function. To do this, we just need to run the following command:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ scorep-score -r scorep_bt-mz_sum/profile.cubex\n"})}),"\n",(0,r.jsx)(n.p,{children:"As an output you will see the following"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Estimated aggregate size of event trace:                   160GB\nEstimated requirements for largest trace buffer (max_buf): 6GB\nEstimated memory requirements (SCOREP_TOTAL_MEMORY):       6GB\n(warning: The memory requirements cannot be satisfied by Score-P to avoid\n intermediate flushes when tracing. Set SCOREP_TOTAL_MEMORY=4G to get the\n maximum supported memory or reduce requirements using USR regions filters.)\n\nflt     type    max_buf[B]        visits time[s] time[%] time/visit[us]  region\n         ALL 6,282,548,755 6,586,867,463 5044.19   100.0           0.77  ALL\n         USR 6,265,237,940 6,574,825,097 2257.25    44.7           0.34  USR\n         OMP    17,537,080    10,975,232 2602.86    51.6         237.16  OMP\n         MPI       985,204       339,446  180.12     3.6         530.62  MPI\n         COM       738,530       727,660    3.93     0.1           5.41  COM\n      SCOREP            41            28    0.03     0.0         934.60  SCOREP\n\n# highlight-start\n         USR 2,014,873,848 2,110,313,472  913.03    18.1           0.43  binvcrhs_\n         USR 2,014,873,848 2,110,313,472  553.30    11.0           0.26  matvec_sub_\n         USR 2,014,873,848 2,110,313,472  718.20    14.2           0.34  matmul_sub_\n         USR    88,951,746    87,475,200   31.80     0.6           0.36  lhsinit_\n         USR    88,951,746    87,475,200   24.24     0.5           0.28  binvrhs_\n         USR    64,926,576    68,892,672   16.66     0.3           0.24  exact_solution_\n# highlight-end\n         OMP     1,398,960       411,648    0.18     0.0           0.43  !$omp parallel @exch_qbc.f:204\n         OMP     1,398,960       411,648    0.18     0.0           0.44  !$omp parallel @exch_qbc.f:215\n         OMP     1,398,960       411,648    0.19     0.0           0.45  !$omp parallel @exch_qbc.f:244\n         OMP     1,398,960       411,648    0.19     0.0           0.45  !$omp parallel @exch_qbc.f:255\n         OMP       702,960       206,848    0.93     0.0           4.49  !$omp parallel @rhs.f:28\n         OMP       699,480       205,824    0.12     0.0           0.57  !$omp parallel @add.f:22\n         OMP       699,480       205,824    0.21     0.0           1.01  !$omp parallel @z_solve.f:43\n         OMP       699,480       205,824    0.21     0.0           1.01  !$omp parallel @x_solve.f:46\n         OMP       699,480       205,824    0.21     0.0           1.02  !$omp parallel @y_solve.f:43\n         MPI       429,336       112,962    0.65     0.0           5.74  MPI_Irecv\n         MPI       429,336       112,962    4.12     0.1          36.48  MPI_Isend\n         OMP       418,080       411,648    2.28     0.0           5.53  !$omp do @exch_qbc.f:204\n         OMP       418,080       411,648    0.55     0.0           1.35  !$omp implicit barrier @exch_qbc.f:213\n         OMP       418,080       411,648    1.75     0.0           4.26  !$omp do @exch_qbc.f:215\n         OMP       418,080       411,648    0.47     0.0           1.14  !$omp implicit barrier @exch_qbc.f:224\n         OMP       418,080       411,648    2.81     0.1           6.82  !$omp do @exch_qbc.f:244\n         OMP       418,080       411,648    0.63     0.0           1.52  !$omp implicit barrier @exch_qbc.f:253\n         OMP       418,080       411,648    2.31     0.0           5.62  !$omp do @exch_qbc.f:255\n         OMP       418,080       411,648    0.52     0.0           1.27  !$omp implicit barrier @exch_qbc.f:264\n         OMP       210,080       206,848    0.44     0.0           2.15  !$omp implicit barrier @rhs.f:439\n         OMP       210,080       206,848   20.74     0.4         100.24  !$omp do @rhs.f:37\n         OMP       210,080       206,848   18.05     0.4          87.25  !$omp do @rhs.f:62\n         OMP       210,080       206,848    1.35     0.0           6.55  !$omp implicit barrier @rhs.f:72\n         OMP       210,080       206,848   31.36     0.6         151.61  !$omp do @rhs.f:80\n         OMP       210,080       206,848   29.51     0.6         142.68  !$omp do @rhs.f:191\n         OMP       210,080       206,848   23.38     0.5         113.02  !$omp do @rhs.f:301\n         OMP       210,080       206,848    5.61     0.1          27.13  !$omp implicit barrier @rhs.f:353\n         OMP       210,080       206,848    0.62     0.0           2.99  !$omp do @rhs.f:359\n         OMP       210,080       206,848    0.46     0.0           2.21  !$omp do @rhs.f:372\n         OMP       210,080       206,848   10.30     0.2          49.80  !$omp do @rhs.f:384\n         OMP       210,080       206,848    0.58     0.0           2.78  !$omp do @rhs.f:400\n         OMP       210,080       206,848    0.39     0.0           1.89  !$omp do @rhs.f:413\n         OMP       210,080       206,848    0.92     0.0           4.44  !$omp implicit barrier @rhs.f:423\n         OMP       210,080       206,848    3.17     0.1          15.33  !$omp do @rhs.f:428\n         OMP       209,040       205,824    8.36     0.2          40.62  !$omp do @add.f:22\n         OMP       209,040       205,824    0.89     0.0           4.35  !$omp implicit barrier @add.f:33\n         OMP       209,040       205,824  185.22     3.7         899.88  !$omp implicit barrier @z_solve.f:428\n         OMP       209,040       205,824  632.18    12.5        3071.45  !$omp do @z_solve.f:52\n         OMP       209,040       205,824  169.53     3.4         823.67  !$omp implicit barrier @x_solve.f:407\n         OMP       209,040       205,824  610.73    12.1        2967.25  !$omp do @x_solve.f:54\n         OMP       209,040       205,824  177.68     3.5         863.28  !$omp implicit barrier @y_solve.f:406\n         OMP       209,040       205,824  638.08    12.6        3100.13  !$omp do @y_solve.f:52\n         COM       209,040       205,824    0.81     0.0           3.91  copy_x_face_\n         COM       209,040       205,824    0.75     0.0           3.63  copy_y_face_\n         MPI       125,424       112,962   93.58     1.9         828.44  MPI_Waitall\n         OMP        52,520        51,712    0.03     0.0           0.60  !$omp master @rhs.f:74\n         OMP        52,520        51,712    0.03     0.0           0.50  !$omp master @rhs.f:183\n         OMP        52,520        51,712    0.02     0.0           0.46  !$omp master @rhs.f:293\n         OMP        52,520        51,712    0.02     0.0           0.30  !$omp master @rhs.f:424\n         COM        52,520        51,712    0.31     0.0           6.09  compute_rhs_\n         COM        52,260        51,456    0.22     0.0           4.27  adi_\n         COM        52,260        51,456    0.36     0.0           6.94  x_solve_\n         COM        52,260        51,456    0.35     0.0           6.73  y_solve_\n         COM        52,260        51,456    0.35     0.0           6.88  z_solve_\n         COM        52,260        51,456    0.29     0.0           5.62  add_\n         USR        37,882        40,796    0.00     0.0           0.10  get_comm_index_\n         OMP         6,960         2,048    0.01     0.0           2.70  !$omp parallel @initialize.f:28\n         COM         5,226         5,628    0.34     0.0          61.16  exch_qbc_\n         OMP         5,200         5,120    0.00     0.0           0.80  !$omp atomic @error.f:51\n         OMP         5,200         5,120    0.00     0.0           0.28  !$omp atomic @error.f:104\n         OMP         3,480         1,024    0.01     0.0           5.34  !$omp parallel @error.f:27\n         OMP         3,480         1,024    0.00     0.0           1.90  !$omp parallel @error.f:86\n         OMP         3,480         1,024    0.00     0.0           1.96  !$omp parallel @exact_rhs.f:21\n         OMP         2,080         2,048    0.04     0.0          18.45  !$omp implicit barrier @initialize.f:204\n         OMP         2,080         2,048    0.19     0.0          94.36  !$omp do @initialize.f:31\n         OMP         2,080         2,048   11.81     0.2        5765.89  !$omp do @initialize.f:50\n         OMP         2,080         2,048    0.06     0.0          30.97  !$omp do @initialize.f:100\n         OMP         2,080         2,048    0.06     0.0          30.76  !$omp do @initialize.f:119\n         OMP         2,080         2,048    0.09     0.0          45.78  !$omp do @initialize.f:137\n         OMP         2,080         2,048    0.09     0.0          45.85  !$omp do @initialize.f:156\n         OMP         2,080         2,048    1.83     0.0         892.68  !$omp implicit barrier @initialize.f:167\n         OMP         2,080         2,048    0.07     0.0          33.67  !$omp do @initialize.f:174\n         OMP         2,080         2,048    0.07     0.0          33.21  !$omp do @initialize.f:192\n         OMP         1,040         1,024    0.15     0.0         143.32  !$omp implicit barrier @error.f:54\n         OMP         1,040         1,024    0.96     0.0         935.38  !$omp do @error.f:33\n         OMP         1,040         1,024    0.00     0.0           2.08  !$omp implicit barrier @error.f:107\n         OMP         1,040         1,024    0.02     0.0          17.27  !$omp do @error.f:91\n         OMP         1,040         1,024    0.00     0.0           2.48  !$omp implicit barrier @exact_rhs.f:357\n         OMP         1,040         1,024    0.21     0.0         201.46  !$omp do @exact_rhs.f:31\n         OMP         1,040         1,024    0.08     0.0          80.42  !$omp implicit barrier @exact_rhs.f:41\n         OMP         1,040         1,024    0.95     0.0         927.89  !$omp do @exact_rhs.f:46\n         OMP         1,040         1,024    1.00     0.0         974.77  !$omp do @exact_rhs.f:147\n         OMP         1,040         1,024    0.51     0.0         501.47  !$omp implicit barrier @exact_rhs.f:242\n         OMP         1,040         1,024    0.98     0.0         956.34  !$omp do @exact_rhs.f:247\n         OMP         1,040         1,024    0.27     0.0         264.42  !$omp implicit barrier @exact_rhs.f:341\n         OMP         1,040         1,024    0.02     0.0          20.34  !$omp do @exact_rhs.f:346\n         MPI           612           252    0.82     0.0        3266.14  MPI_Bcast\n         USR           572           616    0.00     0.0           0.36  timer_clear_\n         COM           520           512    0.02     0.0          47.11  initialize_\n         COM           260           256    0.00     0.0          12.22  exact_rhs_\n         COM           260           256    0.00     0.0           6.18  error_norm_\n         COM           260           256    0.00     0.0           5.87  rhs_norm_\n         MPI           204            84    0.44     0.0        5205.99  MPI_Reduce\n         MPI           136            56    1.36     0.0       24257.44  MPI_Barrier\n         MPI            52            56    0.00     0.0           1.83  MPI_Comm_rank\n      SCOREP            41            28    0.03     0.0         934.60  bt-mz_C.28\n         MPI            26            28    0.00     0.0           4.69  MPI_Comm_size\n         MPI            26            28   29.83     0.6     1065350.67  MPI_Comm_split\n         MPI            26            28    0.01     0.0         352.54  MPI_Finalize\n         MPI            26            28   49.31     1.0     1760964.30  MPI_Init_thread\n         COM            26            28    0.11     0.0        3827.25  MAIN__\n         COM            26            28    0.01     0.0         224.01  mpi_setup_\n         COM            26            28    0.01     0.0         179.26  env_setup_\n         USR            26            28    0.00     0.0          47.11  zone_setup_\n         USR            26            28    0.01     0.0         262.41  map_zones_\n         USR            26            28    0.00     0.0          32.67  zone_starts_\n         USR            26            28    0.00     0.0           1.76  set_constants_\n         USR            26            28    0.00     0.0         117.36  timer_start_\n         USR            26            28    0.00     0.0           8.33  timer_stop_\n         USR            26            28    0.00     0.0           1.11  timer_read_\n         COM            26            28    0.01     0.0         263.89  verify_\n         USR            26             1    0.00     0.0         523.75  print_results_\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The detailed breakdown by region below the summary provides a classification according to these function groups (column type) for each region found in the summary report. Investigation of this part of the score report reveals that most of the trace data would be generated by about 6.8 billion calls to each of the three routines ",(0,r.jsx)(n.code,{children:"binvcrhs"}),", ",(0,r.jsx)(n.code,{children:"matmul_sub"})," and ",(0,r.jsx)(n.code,{children:"matvec_sub"})," (these routines are highlighted), which are classified as ",(0,r.jsx)(n.code,{children:"USR"}),". And although the percentage of time spent in these routines at first glance suggest that they are important, the average time per visit is below 270 nanoseconds (column ",(0,r.jsx)(n.code,{children:"time/visit"}),"). That is, the relative measurement overhead for these functions is substantial, and thus a significant amount of the reported time is very likely spent in the Score-P measurement system rather than in the application itself. Therefore, these routines constitute good candidates for being filtered (like they are good candidates for being inlined by the compiler). Additionally selecting the ",(0,r.jsx)(n.code,{children:"lhsinit"}),", ",(0,r.jsx)(n.code,{children:"binvrhs"}),", and ",(0,r.jsx)(n.code,{children:"exact_solution"})," routines, which generates about 810MB of event data on a single rank with very little runtime impact."]}),"\n",(0,r.jsxs)(n.p,{children:["Score-P allows users to exclude specific routines or files from being measured using a filter file. This file, written in a specific format, specifies what should be included or excluded. In our case, we define rules for certain functions between the keywords ",(0,r.jsx)(n.code,{children:"SCOREP_REGION_NAMES_BEGIN"})," and ",(0,r.jsx)(n.code,{children:"SCOREP_REGION_NAMES_END"}),", the keyword ",(0,r.jsx)(n.code,{children:"EXCLUDE"})," indicating that functions must be excluded from the measurements. A typical Score-P filter file looks like this:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SCOREP_REGION_NAMES_BEGIN\n  EXCLUDE\n    binvcrhs\n    matmul_sub\n    matvec_sub\n    lhsinit\n    binvrhs\n    exact_solution\nSCOREP_REGION_NAMES_END\n"})}),"\n",(0,r.jsxs)(n.p,{children:["We have prepared a filter file ",(0,r.jsx)(n.code,{children:"scorep.filter"}),", which you can find here ",(0,r.jsx)(n.code,{children:"NPB3.3-MZ-MPI/config/scorep.filt"}),". You may notice some differences from the example above, such as the use of asterisks (",(0,r.jsx)(n.code,{children:"*"}),") as bash wildcards, because some Fortran compilers handle ",(0,r.jsx)(n.code,{children:"_"})," symbols in function names differently. We have also excluded timer functions from the measurement."]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"Just to let you know that the filter is safe to use. It doesn't prevent any of the listed routines from being executed. They are simply not recorded in the measurement, so they won't appear in the profile/trace explorer."})}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.p,{children:["Please refer to the Score-P manual ",(0,r.jsx)(n.a,{href:"https://perftools.pages.jsc.fz-juelich.de/cicd/scorep/tags/latest/html/measurement.html#filtering",children:"here"})," for a detailed description of the filter file format, how to filter based on file names, define (and combine) blacklists and whitelists, and how to use wildcards for convenience."]})}),"\n",(0,r.jsxs)(n.p,{children:["The effectiveness of this filter can be examined by scoring the initial summary report again, this time specifying the filter file using the ",(0,r.jsx)(n.code,{children:"-f"})," option of the ",(0,r.jsx)(n.code,{children:"scorep-score -r -f ../config/scorep.filt scorep_bt-mz_sum/profile.cubex"})," command. This way a filter file can be incrementally developed, avoiding the need to conduct many measurements to step-by-step investigate the effect of filtering individual functions."]}),"\n",(0,r.jsx)(n.p,{children:"The output of the aforementioned command will look like this:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Estimated aggregate size of event trace:                   470MB\nEstimated requirements for largest trace buffer (max_buf): 19MB\nEstimated memory requirements (SCOREP_TOTAL_MEMORY):       27MB\n(hint: When tracing set SCOREP_TOTAL_MEMORY=27MB to avoid intermediate flushes\n or reduce requirements using USR regions filters.)\n\nflt     type    max_buf[B]        visits time[s] time[%] time/visit[us]  region\n -       ALL 6,282,548,755 6,586,867,463 5044.19   100.0           0.77  ALL\n -       USR 6,265,237,940 6,574,825,097 2257.25    44.7           0.34  USR\n -       OMP    17,537,080    10,975,232 2602.86    51.6         237.16  OMP\n -       MPI       985,204       339,446  180.12     3.6         530.62  MPI\n -       COM       738,530       727,660    3.93     0.1           5.41  COM\n -    SCOREP            41            28    0.03     0.0         934.60  SCOREP\n\n *       ALL    19,298,841    12,083,275 2786.95    55.3         230.65  ALL-FLT\n +       FLT 6,265,199,954 6,574,784,188 2257.24    44.7           0.34  FLT\n -       OMP    17,537,080    10,975,232 2602.86    51.6         237.16  OMP-FLT\n -       MPI       985,204       339,446  180.12     3.6         530.62  MPI-FLT\n *       COM       738,530       727,660    3.93     0.1           5.41  COM-FLT\n *       USR        38,012        40,909    0.01     0.0           0.34  USR-FLT\n -    SCOREP            41            28    0.03     0.0         934.60  SCOREP-FLT\n\n +       USR 2,014,873,848 2,110,313,472  913.03    18.1           0.43  binvcrhs_\n +       USR 2,014,873,848 2,110,313,472  553.30    11.0           0.26  matvec_sub_\n +       USR 2,014,873,848 2,110,313,472  718.20    14.2           0.34  matmul_sub_\n +       USR    88,951,746    87,475,200   31.80     0.6           0.36  lhsinit_\n +       USR    88,951,746    87,475,200   24.24     0.5           0.28  binvrhs_\n +       USR    64,926,576    68,892,672   16.66     0.3           0.24  exact_solution_\n -       OMP     1,398,960       411,648    0.18     0.0           0.43  !$omp parallel @exch_qbc.f:204\n -       OMP     1,398,960       411,648    0.18     0.0           0.44  !$omp parallel @exch_qbc.f:215\n -       OMP     1,398,960       411,648    0.19     0.0           0.45  !$omp parallel @exch_qbc.f:244\n -       OMP     1,398,960       411,648    0.19     0.0           0.45  !$omp parallel @exch_qbc.f:255\n -       OMP       702,960       206,848    0.93     0.0           4.49  !$omp parallel @rhs.f:28\n -       OMP       699,480       205,824    0.12     0.0           0.57  !$omp parallel @add.f:22\n -       OMP       699,480       205,824    0.21     0.0           1.01  !$omp parallel @z_solve.f:43\n -       OMP       699,480       205,824    0.21     0.0           1.01  !$omp parallel @x_solve.f:46\n -       OMP       699,480       205,824    0.21     0.0           1.02  !$omp parallel @y_solve.f:43\n -       MPI       429,336       112,962    0.65     0.0           5.74  MPI_Irecv\n -       MPI       429,336       112,962    4.12     0.1          36.48  MPI_Isend\n -       OMP       418,080       411,648    2.28     0.0           5.53  !$omp do @exch_qbc.f:204\n -       OMP       418,080       411,648    0.55     0.0           1.35  !$omp implicit barrier @exch_qbc.f:213\n -       OMP       418,080       411,648    1.75     0.0           4.26  !$omp do @exch_qbc.f:215\n -       OMP       418,080       411,648    0.47     0.0           1.14  !$omp implicit barrier @exch_qbc.f:224\n -       OMP       418,080       411,648    2.81     0.1           6.82  !$omp do @exch_qbc.f:244\n -       OMP       418,080       411,648    0.63     0.0           1.52  !$omp implicit barrier @exch_qbc.f:253\n -       OMP       418,080       411,648    2.31     0.0           5.62  !$omp do @exch_qbc.f:255\n -       OMP       418,080       411,648    0.52     0.0           1.27  !$omp implicit barrier @exch_qbc.f:264\n -       OMP       210,080       206,848    0.44     0.0           2.15  !$omp implicit barrier @rhs.f:439\n -       OMP       210,080       206,848   20.74     0.4         100.24  !$omp do @rhs.f:37\n -       OMP       210,080       206,848   18.05     0.4          87.25  !$omp do @rhs.f:62\n -       OMP       210,080       206,848    1.35     0.0           6.55  !$omp implicit barrier @rhs.f:72\n -       OMP       210,080       206,848   31.36     0.6         151.61  !$omp do @rhs.f:80\n -       OMP       210,080       206,848   29.51     0.6         142.68  !$omp do @rhs.f:191\n -       OMP       210,080       206,848   23.38     0.5         113.02  !$omp do @rhs.f:301\n -       OMP       210,080       206,848    5.61     0.1          27.13  !$omp implicit barrier @rhs.f:353\n -       OMP       210,080       206,848    0.62     0.0           2.99  !$omp do @rhs.f:359\n -       OMP       210,080       206,848    0.46     0.0           2.21  !$omp do @rhs.f:372\n -       OMP       210,080       206,848   10.30     0.2          49.80  !$omp do @rhs.f:384\n -       OMP       210,080       206,848    0.58     0.0           2.78  !$omp do @rhs.f:400\n -       OMP       210,080       206,848    0.39     0.0           1.89  !$omp do @rhs.f:413\n -       OMP       210,080       206,848    0.92     0.0           4.44  !$omp implicit barrier @rhs.f:423\n -       OMP       210,080       206,848    3.17     0.1          15.33  !$omp do @rhs.f:428\n -       OMP       209,040       205,824    8.36     0.2          40.62  !$omp do @add.f:22\n -       OMP       209,040       205,824    0.89     0.0           4.35  !$omp implicit barrier @add.f:33\n -       OMP       209,040       205,824  185.22     3.7         899.88  !$omp implicit barrier @z_solve.f:428\n -       OMP       209,040       205,824  632.18    12.5        3071.45  !$omp do @z_solve.f:52\n -       OMP       209,040       205,824  169.53     3.4         823.67  !$omp implicit barrier @x_solve.f:407\n -       OMP       209,040       205,824  610.73    12.1        2967.25  !$omp do @x_solve.f:54\n -       OMP       209,040       205,824  177.68     3.5         863.28  !$omp implicit barrier @y_solve.f:406\n -       OMP       209,040       205,824  638.08    12.6        3100.13  !$omp do @y_solve.f:52\n -       COM       209,040       205,824    0.81     0.0           3.91  copy_x_face_\n -       COM       209,040       205,824    0.75     0.0           3.63  copy_y_face_\n -       MPI       125,424       112,962   93.58     1.9         828.44  MPI_Waitall\n -       OMP        52,520        51,712    0.03     0.0           0.60  !$omp master @rhs.f:74\n -       OMP        52,520        51,712    0.03     0.0           0.50  !$omp master @rhs.f:183\n -       OMP        52,520        51,712    0.02     0.0           0.46  !$omp master @rhs.f:293\n -       OMP        52,520        51,712    0.02     0.0           0.30  !$omp master @rhs.f:424\n -       COM        52,520        51,712    0.31     0.0           6.09  compute_rhs_\n -       COM        52,260        51,456    0.22     0.0           4.27  adi_\n -       COM        52,260        51,456    0.36     0.0           6.94  x_solve_\n -       COM        52,260        51,456    0.35     0.0           6.73  y_solve_\n -       COM        52,260        51,456    0.35     0.0           6.88  z_solve_\n -       COM        52,260        51,456    0.29     0.0           5.62  add_\n -       USR        37,882        40,796    0.00     0.0           0.10  get_comm_index_\n -       OMP         6,960         2,048    0.01     0.0           2.70  !$omp parallel @initialize.f:28\n -       COM         5,226         5,628    0.34     0.0          61.16  exch_qbc_\n -       OMP         5,200         5,120    0.00     0.0           0.80  !$omp atomic @error.f:51\n -       OMP         5,200         5,120    0.00     0.0           0.28  !$omp atomic @error.f:104\n -       OMP         3,480         1,024    0.01     0.0           5.34  !$omp parallel @error.f:27\n -       OMP         3,480         1,024    0.00     0.0           1.90  !$omp parallel @error.f:86\n -       OMP         3,480         1,024    0.00     0.0           1.96  !$omp parallel @exact_rhs.f:21\n -       OMP         2,080         2,048    0.04     0.0          18.45  !$omp implicit barrier @initialize.f:204\n -       OMP         2,080         2,048    0.19     0.0          94.36  !$omp do @initialize.f:31\n -       OMP         2,080         2,048   11.81     0.2        5765.89  !$omp do @initialize.f:50\n -       OMP         2,080         2,048    0.06     0.0          30.97  !$omp do @initialize.f:100\n -       OMP         2,080         2,048    0.06     0.0          30.76  !$omp do @initialize.f:119\n -       OMP         2,080         2,048    0.09     0.0          45.78  !$omp do @initialize.f:137\n -       OMP         2,080         2,048    0.09     0.0          45.85  !$omp do @initialize.f:156\n -       OMP         2,080         2,048    1.83     0.0         892.68  !$omp implicit barrier @initialize.f:167\n -       OMP         2,080         2,048    0.07     0.0          33.67  !$omp do @initialize.f:174\n -       OMP         2,080         2,048    0.07     0.0          33.21  !$omp do @initialize.f:192\n -       OMP         1,040         1,024    0.15     0.0         143.32  !$omp implicit barrier @error.f:54\n -       OMP         1,040         1,024    0.96     0.0         935.38  !$omp do @error.f:33\n -       OMP         1,040         1,024    0.00     0.0           2.08  !$omp implicit barrier @error.f:107\n -       OMP         1,040         1,024    0.02     0.0          17.27  !$omp do @error.f:91\n -       OMP         1,040         1,024    0.00     0.0           2.48  !$omp implicit barrier @exact_rhs.f:357\n -       OMP         1,040         1,024    0.21     0.0         201.46  !$omp do @exact_rhs.f:31\n -       OMP         1,040         1,024    0.08     0.0          80.42  !$omp implicit barrier @exact_rhs.f:41\n -       OMP         1,040         1,024    0.95     0.0         927.89  !$omp do @exact_rhs.f:46\n -       OMP         1,040         1,024    1.00     0.0         974.77  !$omp do @exact_rhs.f:147\n -       OMP         1,040         1,024    0.51     0.0         501.47  !$omp implicit barrier @exact_rhs.f:242\n -       OMP         1,040         1,024    0.98     0.0         956.34  !$omp do @exact_rhs.f:247\n -       OMP         1,040         1,024    0.27     0.0         264.42  !$omp implicit barrier @exact_rhs.f:341\n -       OMP         1,040         1,024    0.02     0.0          20.34  !$omp do @exact_rhs.f:346\n -       MPI           612           252    0.82     0.0        3266.14  MPI_Bcast\n +       USR           572           616    0.00     0.0           0.36  timer_clear_\n -       COM           520           512    0.02     0.0          47.11  initialize_\n -       COM           260           256    0.00     0.0          12.22  exact_rhs_\n -       COM           260           256    0.00     0.0           6.18  error_norm_\n -       COM           260           256    0.00     0.0           5.87  rhs_norm_\n -       MPI           204            84    0.44     0.0        5205.99  MPI_Reduce\n -       MPI           136            56    1.36     0.0       24257.44  MPI_Barrier\n -       MPI            52            56    0.00     0.0           1.83  MPI_Comm_rank\n -    SCOREP            41            28    0.03     0.0         934.60  bt-mz_C.28\n -       MPI            26            28    0.00     0.0           4.69  MPI_Comm_size\n -       MPI            26            28   29.83     0.6     1065350.67  MPI_Comm_split\n -       MPI            26            28    0.01     0.0         352.54  MPI_Finalize\n -       MPI            26            28   49.31     1.0     1760964.30  MPI_Init_thread\n -       COM            26            28    0.11     0.0        3827.25  MAIN__\n -       COM            26            28    0.01     0.0         224.01  mpi_setup_\n -       COM            26            28    0.01     0.0         179.26  env_setup_\n -       USR            26            28    0.00     0.0          47.11  zone_setup_\n -       USR            26            28    0.01     0.0         262.41  map_zones_\n -       USR            26            28    0.00     0.0          32.67  zone_starts_\n -       USR            26            28    0.00     0.0           1.76  set_constants_\n +       USR            26            28    0.00     0.0         117.36  timer_start_\n +       USR            26            28    0.00     0.0           8.33  timer_stop_\n +       USR            26            28    0.00     0.0           1.11  timer_read_\n -       COM            26            28    0.01     0.0         263.89  verify_\n -       USR            26             1    0.00     0.0         523.75  print_results_\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Below the (original) function group summary, the score report now also includes a second summary with the filter applied. Here, an additional group ",(0,r.jsx)(n.code,{children:"FLT"})," is added, which subsumes all filtered regions. Moreover, the column ",(0,r.jsx)(n.code,{children:"flt"})," indicates whether a region/function group is filtered (",(0,r.jsx)(n.code,{children:"+"}),"), not filtered (",(0,r.jsx)(n.code,{children:"-"}),"), or possibly partially filtered (",(0,r.jsx)(n.code,{children:"\u2217"}),", only used for function groups)."]}),"\n",(0,r.jsxs)(n.p,{children:["As expected, the estimate for the aggregate event trace size drops down to 470MB, and the process-local maximum across all ranks is reduced to 19MB. Since the Score-P measurement system also creates a number of internal data structures (e.g., to track MPI requests and communicators), the suggested setting for the ",(0,r.jsx)(n.code,{children:"SCOREP_TOTAL_MEMORY"})," environment variable to adjust the maximum amount of memory used by the Score-P memory management is 27MB when tracing is configured."]}),"\n",(0,r.jsx)(n.p,{children:":::"}),"\n",(0,r.jsxs)(n.p,{children:["With the ",(0,r.jsx)(n.code,{children:"-g"})," option, ",(0,r.jsx)(n.code,{children:"scorep-score"})," can create an initial filter file in Score-P format. See more details ",(0,r.jsx)(n.a,{href:"https://perftools.pages.jsc.fz-juelich.de/cicd/scorep/tags/latest/html/score.html",children:"here"}),"."]}),"\n",(0,r.jsx)(n.p,{children:":::"}),"\n",(0,r.jsxs)(n.p,{children:["Let's modify our batch script ",(0,r.jsx)(n.code,{children:"score.sbatch"})," to enable filtering (see highlighted lines):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",metastring:"showLineNumbers",children:"#!/bin/bash\n#SBATCH -o bt-mz.%j.out\n#SBATCH -e bt-mz.%j.err\n#SBATCH -J bt-mz\n#SBATCH --clusters=cm2_tiny\n#SBATCH --partition=cm2_tiny\n#SBATCH --reservation=hhps1s24\n#SBATCH --nodes=2\n#SBATCH --ntasks=28\n#SBATCH --ntasks-per-node=14\n#SBATCH --get-user-env\n#SBATCH --time=00:05:00\n\nmodule use /lrz/sys/courses/vihps/2024/modulefiles/\nmodule load scorep/8.4-intel-intelmpi\nexport OMP_NUM_THREADS=4\n\n# Score-P measurement configuration\n# highlight-start\nexport SCOREP_EXPERIMENT_DIRECTORY=scorep_bt-mz_sum_filt\nexport SCOREP_FILTERING_FILE=../config/scorep.filt\n# highlight-start\n\n# Benchmark configuration (disable load balancing with threads)\nexport NPB_MZ_BLOAD=0\nPROCS=28\nCLASS=C\n\n# Run the application\nmpiexec -n $SLURM_NTASKS ./bt-mz_$CLASS.$PROCS\n"})}),"\n",(0,r.jsxs)(n.p,{children:["In first highlighted line we added suffix ",(0,r.jsx)(n.code,{children:"_filt"})," to create measurement directory with a different name. In the second one we provided name of the filter file which will be used during the measurement."]}),"\n",(0,r.jsxs)(n.admonition,{type:"info",children:[(0,r.jsxs)(n.p,{children:["If you do not specify ",(0,r.jsx)(n.code,{children:"SCOREP_EXPERIMENT_DIRECTORY"})," variable, the experiment directory is named in the format ",(0,r.jsx)(n.code,{children:"scorep-YYYYMMDD_HHMM_XXXXXXXX"}),", where ",(0,r.jsx)(n.code,{children:"YYYYMMDD"})," and ",(0,r.jsx)(n.code,{children:"HHMM"})," represent the date and time, followed by random numbers."]}),(0,r.jsxs)(n.p,{children:["If a directory with the specified name already exists, it will be renamed with a date suffix by default. To prevent this and abort the measurement if the directory exists, set ",(0,r.jsx)(n.code,{children:"SCOREP_OVERWRITE_EXPERIMENT_DIRECTORY"})," to ",(0,r.jsx)(n.code,{children:"false"}),". This setting is effective only if ",(0,r.jsx)(n.code,{children:"SCOREP_EXPERIMENT_DIRECTORY"})," is set."]})]}),"\n",(0,r.jsx)(n.p,{children:"Now we are ready to submit our batch script with enabled filtering"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ sbatch scorep.sbatch\n"})}),"\n",(0,r.jsx)(n.admonition,{title:"Question",type:"tip",children:(0,r.jsxs)(n.p,{children:['Open the freshly generated stdout file and find the metric "Time in seconds". Compare it to our baseline measurement ',(0,r.jsx)(n.a,{href:"/perf-analysis-hands-on/docs/baseline",children:"here"})," and our original instrumented run ",(0,r.jsx)(n.a,{href:"/perf-analysis-hands-on/docs/instrumentation",children:"here"}),". Has it increased or decreased? If so, by how much? Which routines in your opinion are safe to filter?"]})})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var r=i(6540);const t={},o=r.createContext(t);function s(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);